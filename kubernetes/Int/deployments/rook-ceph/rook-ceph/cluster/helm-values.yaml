monitoring:
  enabled: false
toolbox:
  enabled: true
  ## Enable to run toolbox as root
  # containerSecurityContext:
  #   runAsNonRoot: false
  #   runAsUser: 0
  #   runAsGroup: 0
  #   capabilities:
  #      drop: ["NONE"]
configOverride: |
  [global]
  bdev_enable_discard = true
  bdev_async_discard = true
  osd_class_update_on_start = false
cephClusterSpec:
  crashCollector:
    disable: false
  dashboard:
    enabled: false
  mgr:
    modules:
      - name: pg_autoscaler
        enabled: true
  mon:
    count: 1
  network:
    provider: host
    addressRanges:
      public:
        - "172.16.0.0/24"
  storage:
    useAllNodes: false
    useAllDevices: false
    deviceFilter:
    config:
      osdsPerDevice: "1"
    nodes:
    # - name: kc-c-01
    #   devices:
    #   - name: "/dev/disk/by-id/nvme-CT2000P3PSSD8_2423E8B70EA3"
    - name: kc-c-02
      devices:
      - name: "/dev/disk/by-id/nvme-Samsung_SSD_980_PRO_2TB_S6B0NL0TC09703D"
    # - name: kc-c-03
    #   devices:
    #   - name: "/dev/disk/by-id/nvme-Samsung_SSD_980_PRO_2TB_S6B0NL0TC08560R"
    - name: k-w-02
      devices:
      - name: "/dev/disk/by-id/nvme-Samsung_SSD_980_PRO_2TB_S6B0NL0W101854X"
    - name: k-w-04
      devices:
      - name: "/dev/disk/by-id/nvme-Samsung_SSD_990_PRO_1TB_S73VNJ0W708742X"
    - name: k-w-06
      devices:
      - name: "/dev/disk/by-id/nvme-Samsung_SSD_990_PRO_1TB_S73VNJ0W320254A"
    - name: int-n-01
      devices:
      - name: "/dev/disk/by-id/nvme-Samsung_SSD_990_PRO_1TB_S73VNJ0W708745J"
      - name: "/dev/disk/by-id/nvme-Samsung_SSD_990_PRO_1TB_S73VNJ0W320270E"
      - name: "/dev/disk/by-id/nvme-Samsung_SSD_980_1TB_S64ANJ0R931812B"
      - name: "/dev/disk/by-id/nvme-CT1000P5PSSD8_22503D27B049"
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: node-role.kubernetes.io/storage
                  operator: Exists
  resources:
    mgr:
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        memory: 2Gi
    mon:
      requests:
        cpu: 50m
        memory: 512Mi
      limits:
        memory: 1Gi
    osd:
      requests:
        cpu: 500m
        memory: 2Gi
      limits:
        memory: 6Gi
    mgr-sidecar:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        memory: 256Mi
cephBlockPools:
  - name: ceph-blockpool
    spec:
      failureDomain: host
      replicated:
        size: 3
    storageClass:
      enabled: true
      name: ceph-block
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: Immediate
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4
cephBlockPoolsVolumeSnapshotClass:
  enabled: true
  name: csi-ceph-blockpool
  isDefault: false
  deletionPolicy: Delete
cephFileSystems:
  - name: &cephFileSystemName ceph-filesystem
    spec:
      metadataPool:
        replicated:
          size: 3
      dataPools:
        - failureDomain: host
          replicated:
            size: 3
          name: data0
      metadataServer:
        activeCount: 1
        activeStandby: true
        priorityClassName: system-cluster-critical
        placement:
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: ceph-mds
                  app.kubernetes.io/part-of: *cephFileSystemName
        resources:
          requests:
            cpu: 100m
            memory: 1Gi
          limits:
            memory: 4Gi
    storageClass:
      enabled: true
      isDefault: false
      name: ceph-filesystem
      pool: data0
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: Immediate
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4
cephFileSystemVolumeSnapshotClass:
  enabled: true
  name: csi-ceph-filesystem
  isDefault: false
  deletionPolicy: Delete
cephObjectStores:
  - name: ceph-objectstore
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 3
      dataPool:
        failureDomain: host
        erasureCoded:
          dataChunks: 2
          codingChunks: 1
      preservePoolsOnDelete: true
      gateway:
        hostNetwork: false
        port: 80
        resources:
          requests:
            cpu: 100m
            memory: 1Gi
          limits:
            memory: 2Gi
        instances: 2
        priorityClassName: system-cluster-critical
      healthCheck:
        bucket:
          interval: 60s
    storageClass:
      enabled: true
      name: ceph-bucket
      reclaimPolicy: Delete
      volumeBindingMode: Immediate
      parameters:
        region: us-east-1
